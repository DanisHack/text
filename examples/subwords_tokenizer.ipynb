{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/examples/subwords_tokenizer.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/examples/subwords_tokenizer.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/text/examples/subwords_tokenizer.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIMuBnQO6ZoV"
      },
      "source": [
        "The `tensorflow_text` package includes a TensorFlow implementation of a subword tokenizer in `text.BertTokenizer`. This tutorial demonstrates how to generate a subword vocabulary from a dataset, and build a `text.BertTokenizer` from the vocabulary.\n",
        "\n",
        "The main advantage of a subword tokenizer is that it interpolates between word-based and character-based tokenization. Common words get a slot in the vocabulary, but the tokenizer can fall back to word pieces and individual characters for unknown words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFG0NDRu5mYQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZi9RstHxO_Z"
      },
      "outputs": [],
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJbGA5N5mXr"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC9TeTd47j8p"
      },
      "source": [
        "Fetch the Portugese/English translation dataset from [tfds](https://tensorflow.org/datasets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDaAOTKHNy8e"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GHc3O2W8Hgg"
      },
      "source": [
        "This dataset produces Portugese/English sentence pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ezZT8w8GqD"
      },
      "outputs": [],
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portugese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGwm45vKttj"
      },
      "source": [
        "Note a few things about the example sentences above:\n",
        "* They're lower case.\n",
        "* There are spaces around the punctuation.\n",
        "* It's not clear if or what unicode normalization is being used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYPhi63P70ei"
      },
      "source": [
        "## Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TN9KUcJKquS"
      },
      "source": [
        "The standardization function below converts raw text to the same format as the dataset, and normalizes them using [Compatibility decomposition `NFKD`](https://unicode.org/reports/tr15/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHFegKAl70Fz"
      },
      "outputs": [],
      "source": [
        "def standardize_text(sentences):\n",
        "  # Make everything lowercase.\n",
        "  sentences = tf.strings.lower(sentences)\n",
        "  # Insert spaces before punctuation\n",
        "  sentences = tf.strings.regex_replace(\n",
        "      sentences, '[%s]' % re.escape(string.punctuation), r' \\0 ')\n",
        "  # Normalize unicode.\n",
        "  sentences = text.normalize_utf8(sentences, 'NFKD')  \n",
        "  # Collapse multiple spaces into one.\n",
        "  sentences = tf.strings.regex_replace(\n",
        "      sentences,'\\s+', r' ')\n",
        "  # Drop any spaces at the start or end.\n",
        "  sentences = tf.strings.regex_replace(\n",
        "      sentences,'(^\\s|\\s$)', r'')\n",
        "  \n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fQJq1xB-tOn"
      },
      "outputs": [],
      "source": [
        "example = \"Da impress√£o,\".encode('utf-8')\n",
        "\n",
        "print(\"Before: \", example)\n",
        "print(\"After:  \", standardize_text(example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8pV6h0FsZZj"
      },
      "source": [
        "## Word counts\n",
        "\n",
        "To generate the vocabulary for the `BertTokenizer` you'll need to extract the word-counts from the dataset for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KsRQTe_nNgm"
      },
      "source": [
        "Use a python [collections.Counter](https://docs.python.org/2/library/collections.html#collections.Counter) to do the counting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGMRbeRQUn2h"
      },
      "outputs": [],
      "source": [
        "def count_words(ds):\n",
        "  counts = collections.Counter()\n",
        "  # Batch for efficiency\n",
        "  for sentences in ds.batch(1024).map(standardize_text).prefetch(1):\n",
        "    # Split on spaces, the result is a RaggedTensor\n",
        "    sentences = tf.strings.split(sentences, sep=\" \")\n",
        "    # Convert the RaggedTensor to a list of lists.\n",
        "    sentences = sentences.to_list()\n",
        "    # Dor each list of words, \n",
        "    for words in sentences:\n",
        "      # add to the word counts.\n",
        "      counts.update(words)\n",
        "\n",
        "  # Decode the words.\n",
        "  counts = collections.Counter({\n",
        "      word.decode('utf-8'): count\n",
        "      for word, count in counts.items()})\n",
        "  return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAdkg3lJtW9a"
      },
      "outputs": [],
      "source": [
        "def select_pt(pt, en):\n",
        "  return pt\n",
        "\n",
        "counts = count_words(train_examples.map(select_pt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBd7Hmy_4Uhe"
      },
      "source": [
        "Now you have word counts for the Portugese part of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L76cu88oevc"
      },
      "outputs": [],
      "source": [
        "print(f\"Unique words: {len(counts)}\\n\")\n",
        "print(f\"Most common Portugese words and counts:\")\n",
        "for word, count  in counts.most_common(20):\n",
        "  print(f\"    {word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUdUH8FmET66"
      },
      "source": [
        "## Subwords vocabulary generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R74W3QabgWmX"
      },
      "source": [
        "The vocabulary generation code is not included in the `tensorflow_text` pip package. So clone the repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Nhvt-qMfNp"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/text/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM-tR_7X-jtW"
      },
      "source": [
        "The code of interest is in the `test/tools` directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-xgHqj1-pB6"
      },
      "outputs": [],
      "source": [
        "!ls text/tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnc9ANBh_OMU"
      },
      "source": [
        "So add the tools directory to the python path, so you can import modules from there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc-TjSMI8eK5"
      },
      "outputs": [],
      "source": [
        "if str(pwd/\"text/tools\") not in sys.path:\n",
        "  sys.path.append(str(pwd/\"text/tools\"))\n",
        "\n",
        "from wordpiece_vocab import wordpiece_tokenizer_learner_lib as vocab_learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWSnj8xFgI7"
      },
      "source": [
        "The `learner` takes a bunch of parameters as input.\n",
        "\n",
        "The `upper_threshold` and `lower_threshold` parameters set the initial limits of the binary search as it searches for a vocabulary near the requested `vocab_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcDkLSxI_hHt"
      },
      "outputs": [],
      "source": [
        "default_params = dict(    \n",
        "    upper_thresh=10000000,\n",
        "    lower_thresh=10,\n",
        "    num_iterations=4,\n",
        "    max_input_tokens=5000000,\n",
        "    max_token_length=50,\n",
        "    max_unique_chars=1000,\n",
        "    slack_ratio=0.05,\n",
        "    include_joiner_token=True, \n",
        "    joiner=\"##\")\n",
        "\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "params = vocab_learner.Params(\n",
        "    vocab_size=8000, \n",
        "    reserved_tokens=reserved_tokens,\n",
        "    **default_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gTty2Wh-dHm"
      },
      "source": [
        "This takes about 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cBBESOfi-uq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "pt_vocab = vocab_learner.learn(counts.items(), params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cl4d2O34gkH"
      },
      "source": [
        "Here are some slices of the resulting vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfaPmX54FvhW"
      },
      "outputs": [],
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owkP3wbYVQv0"
      },
      "source": [
        "Write a vocabulary file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY6v1ThkKDyZ"
      },
      "outputs": [],
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_TR5U1xWvAV"
      },
      "outputs": [],
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqm34KMIVoRK"
      },
      "source": [
        "Now put that all together into a single function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLzm-U2RVty8"
      },
      "outputs": [],
      "source": [
        "def gen_subwords_vocab(ds, filepath):\n",
        "  counts = count_words(ds)\n",
        "\n",
        "  params = vocab_learner.Params(\n",
        "    vocab_size=8000, \n",
        "    reserved_tokens=reserved_tokens,\n",
        "    **default_params)\n",
        "\n",
        "  vocab = vocab_learner.learn(counts.items(), params)\n",
        "  write_vocab_file(filepath, vocab)\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ag3qcx54nii"
      },
      "source": [
        "Use that function to generate a vocabulart from the english data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "height": 409
        },
        "executionInfo": {
          "elapsed": 1166,
          "status": "error",
          "timestamp": 1603732165504,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "R3cMumvHWWtl",
        "outputId": "201504e0-a89c-41fe-9c07-bc18ca4be489"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-15-914ae5a235fe\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"def select_en(pt, en):\\n  return en\\n\\nen_examples = train_examples.map(select_en)\\nen_vocab = gen_subwords_vocab(en_examples, 'en_vocab.txt')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_brain_frameworks_cpu_markdaoust.kernel.markdaoust.1074049716914.14b334fb3717c109/mount/server/ml_notebook.runfiles/google3/third_party/py/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2179\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2180\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-\u003e 2181\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_brain_frameworks_cpu_markdaoust.kernel.markdaoust.1074049716914.14b334fb3717c109/mount/server/ml_notebook.runfiles/google3/research/colab/lib/_debugger.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0m_add_to_linecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\u003ctimed exec\u003e'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m       \u001b[0m_add_to_linecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\u003ctimed eval\u003e'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 94\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mColabExecutionMagics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_debug_post_mortem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003c/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_brain_frameworks_cpu_markdaoust.kernel.markdaoust.1074049716914.14b334fb3717c109/mount/server/ml_notebook.runfiles/google3/third_party/py/decorator/__init__.py:decorator-gen-60\u003e\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_brain_frameworks_cpu_markdaoust.kernel.markdaoust.1074049716914.14b334fb3717c109/mount/server/ml_notebook.runfiles/google3/third_party/py/IPython/core/magic.py\u001b[0m in \u001b[0;36m\u003clambda\u003e\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_brain_frameworks_cpu_markdaoust.kernel.markdaoust.1074049716914.14b334fb3717c109/mount/server/ml_notebook.runfiles/google3/third_party/py/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-\u003e 1136\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003ctimed exec\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0men_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 5\u001b[1;33m \u001b[0men_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_subwords_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en_vocab.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'gen_subwords_vocab' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def select_en(pt, en):\n",
        "  return en\n",
        "\n",
        "en_examples = train_examples.map(select_en)\n",
        "en_vocab = gen_subwords_vocab(en_examples, 'en_vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxOpzMd8ol5B"
      },
      "outputs": [],
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3LG_f34wCs"
      },
      "source": [
        "Here are the two vocabulary files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "executionInfo": {
          "elapsed": 444,
          "status": "ok",
          "timestamp": 1603732194946,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "djehfEL6Zn-I",
        "outputId": "ecc650c2-ef1f-4bd2-d177-cf6fcf02bdd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access *.txt: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls *.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb5ddYLTBJhk"
      },
      "source": [
        "## Build the BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgp5gvR-2tQ"
      },
      "source": [
        "The `text.BertTokenizer` can be initialized by passing the vocabulary file's path as the first argument: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdMpt9ZEjVGu"
      },
      "outputs": [],
      "source": [
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt')\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPZafCUds86"
      },
      "source": [
        "Now you can use it to encode some text. Initially this returns a `tf.RaggedTensor` with axes `(batch, word, word-piece)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKF0QJjtUm9T"
      },
      "outputs": [],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeTM81lAc8q1"
      },
      "outputs": [],
      "source": [
        "enc = standardize_text(en_examples)\n",
        "enc = en_tokenizer.tokenize(enc)\n",
        "\n",
        "for ex in enc.to_list():\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTnWUKnPELgV"
      },
      "source": [
        "Merge the `word` and `word-piece` axes into a single `word-piece` axis for the whole sentence, using `RaggedTensor.merge_dims`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLxGtRIL6fvb"
      },
      "outputs": [],
      "source": [
        "enc = enc.merge_dims(1,2)\n",
        "\n",
        "for ex in enc.to_list():\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUR9hHj0PUy"
      },
      "source": [
        "Add the `[START]` and `[END]` tokens (they're at the same indexes for both languages):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyyoa5De0WQu"
      },
      "outputs": [],
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "height": 215
        },
        "executionInfo": {
          "elapsed": 409,
          "status": "error",
          "timestamp": 1603732528607,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "MrZjQIwZ6NHu",
        "outputId": "b05f07b4-9468-41f2-add6-37ab0784cd2f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-17-2091f021d146\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_start_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'add_start_end' is not defined"
          ]
        }
      ],
      "source": [
        "enc = add_start_end(enc)\n",
        "\n",
        "for ex in enc.to_list():\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbBo_VUC76Ru"
      },
      "source": [
        "Many models can't handle `tf.ragged` directly but you can convert to a padded dense tensor using `RaggedTensor.to_tensor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhfeHznTEYZH"
      },
      "outputs": [],
      "source": [
        "enc.to_tensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67VUpyyNFHjt"
      },
      "source": [
        "Finally, put those steps together into a single module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "executionInfo": {
          "elapsed": 393,
          "status": "ok",
          "timestamp": 1603732548667,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "PVwrJ_QJ6n2e"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(tf.Module):\n",
        "  def __init__(self, vocab):\n",
        "    self.tokenizer = text.BertTokenizer(vocab)\n",
        "    \n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
        "  def encode(self, strings):\n",
        "    enc = standardize_text(strings)\n",
        "    enc = self.tokenizer.tokenize(enc)\n",
        "    enc = enc.merge_dims(1,2)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTesKEOy6MPi"
      },
      "source": [
        "Build one of these text-encoders for each language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnxEuExUFGGL"
      },
      "outputs": [],
      "source": [
        "encoders = tf.Module()\n",
        "encoders.pt = TextEncoder(\"pt_vocab.txt\")\n",
        "encoders.en = TextEncoder(\"en_vocab.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVYgyDWU6Ry0"
      },
      "source": [
        "Try the encode function in the batch of english examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0h0INYFsYlw"
      },
      "outputs": [],
      "source": [
        "encoders.en.encode(en_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMUGS-9gGaP"
      },
      "source": [
        "Since `text.BertTokenizer` is entierly implemented in TensorFlow, it can be exported and restored as part of a `saved_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VR3e3xIhMfk"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(encoders, 'encoders')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdPloX4gh16E"
      },
      "outputs": [],
      "source": [
        "reloaded_encoders = tf.saved_model.load('encoders')\n",
        "reloaded_encoders.en.encode(en_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMmHS5VT_suH"
      },
      "source": [
        "## Text decoding\n",
        "\n",
        "Sometimes you may want to generate text, and need to convert back from tokens IDs to human-readable text.\n",
        "\n",
        "For that you need an inverse vocabulary table. There are lots of ways you could implement this. If you stick with a pure TensorFlow implementation you can export the text decoding with your model too.\n",
        "\n",
        "Since the IDs are dense, you can just pack the tokens into a string tensor, and use `tf.gather` to look them up. Otherwise see `tf.lookup.StaticHashTable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXmUVCdYjkKj"
      },
      "outputs": [],
      "source": [
        "en_vocab = pathlib.Path('en_vocab.txt').read_text().splitlines()\n",
        "en_vocab = tf.Variable(en_vocab)\n",
        "tokens = tf.gather(en_vocab, enc)\n",
        "for line in tokens.to_list():\n",
        "  print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju4CdI7aoAi2"
      },
      "source": [
        "With a little more processing, it can be made human readable again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4QdIq76MQ6V"
      },
      "outputs": [],
      "source": [
        "def decode_text(vocab, encoded):\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "  \n",
        "  # Lookup the tokens.\n",
        "  result = tf.gather(vocab, encoded)\n",
        "\n",
        "  # Drop the reserved tokens\n",
        "  bad_cells = tf.strings.regex_full_match(result, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(result, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  # Fix the subword-joiners\n",
        "  result = tf.strings.regex_replace(result, ' ##', '',)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB3MJhNvkuBb"
      },
      "outputs": [],
      "source": [
        "for line in decode_text(en_vocab, enc):\n",
        "  print(line.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFuo1KZjpEPR"
      },
      "source": [
        "Putting it all together, you can build a 100% self contained encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUUor5TtpLZc"
      },
      "outputs": [],
      "source": [
        "class TextConverter(TextEncoder):\n",
        "  def __init__(self, vocab):\n",
        "    super().__init__(vocab)\n",
        "    vocab = pathlib.Path(vocab).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    # Include signatures for both Tensor and RaggedTensor inputs.\n",
        "    self.decode.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.decode.get_concrete_function(\n",
        "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "  \n",
        "    self.get_tokens.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.get_tokens.get_concrete_function(\n",
        "        tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "  @tf.function\n",
        "  def decode(self, encoded):\n",
        "    return decode_text(self.vocab, encoded)\n",
        "\n",
        "  @tf.function\n",
        "  def get_tokens(self, ids):\n",
        "    return tf.gather(self.vocab, ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHzEnTQM6nBD"
      },
      "source": [
        "Build an encoder/decoder for each language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU8yFBCSruz4"
      },
      "outputs": [],
      "source": [
        "converters = tf.Module()\n",
        "converters.pt = TextConverter('pt_vocab.txt')\n",
        "converters.en = TextConverter('en_vocab.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYfrmDhy6syT"
      },
      "source": [
        "Export them as a `saved_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "height": 198
        },
        "executionInfo": {
          "elapsed": 336,
          "status": "error",
          "timestamp": 1603732704576,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "aieDGooa9ms7",
        "outputId": "de8235c6-47b4-404c-a062-eccca08956b0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-19-0ec1399d9e60\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ted_hrlr_translate_pt_en_converter'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mreloaded_converters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'converters' is not defined"
          ]
        }
      ],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.saved_model.save(converters, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoCMz2Fm61v6"
      },
      "source": [
        "Reload the `saved_model` and test the methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "executionInfo": {
          "elapsed": 872,
          "status": "ok",
          "timestamp": 1603732716474,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "9SB_BHwqsHkb",
        "outputId": "dcf4e42c-9eda-486e-9f69-8a17266fe767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4014, 2364,  697, 1199, 2372,    4]])"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reloaded_converters = tf.saved_model.load(model_name)\n",
        "enc = reloaded_converters.en.encode(['Hello TensorFlow!'])\n",
        "enc.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afj2fJxXdF7d"
      },
      "outputs": [],
      "source": [
        "tokens = reloaded_converters.en.get_tokens(enc)\n",
        "tokens.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0205N_8dDT5"
      },
      "outputs": [],
      "source": [
        "round_trip = reloaded_converters.en.decode(enc)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSKFDQoBjnNp"
      },
      "source": [
        "Archive it for the [translation tutorials](https://tensorflow.org/tutorials/text/transformer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY0SoE3Yj2it"
      },
      "outputs": [],
      "source": [
        "!zip -r {model_name}.zip {model_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Synq0RekAXe"
      },
      "outputs": [],
      "source": [
        "!du -sh *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtmGkGBuGHa2"
      },
      "source": [
        "## Optional: The algorithm\n",
        "\n",
        "It's worth noting here that there are two versions of the WordPiece algorithm: Bottom-up and top-down. In both cases goal is the same: \"Given a training corpus and a number of desired\n",
        "tokens D, the optimization problem is to select D wordpieces such that the resulting corpus is minimal in the\n",
        "number of wordpieces when segmented according to the chosen wordpiece model.\"\n",
        "\n",
        "The  original [bottom-up WordPiece algorithm](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), is based on [byte-pair encoding](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10). Like BPE It starts with the alphabet, and iteratively combines common bigrams to form word-pieces and words. \n",
        "\n",
        "TensorFlow text's vocabulary generator follows the top-down implementation from [BERT](https://arxiv.org/pdf/1810.04805.pdf). Starting with words and breaking them down into smaller components until they hit the frequency threshold, or can't be broken down further. The next section describes this in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLA2QhffYEo0"
      },
      "source": [
        "### Choosing the vocabulary\n",
        "\n",
        "The top-down WordPiece generation algorithm takes in a set of (word, count) pairs and a threshold `T`, and returns a vocabulary `V`.\n",
        "\n",
        "The algorithm is iterative. It is run for `k` iterations, where typically `k = 4`, but only the first two are really important. The third and fourth (and beyond) are just identical to the second. Note that each step of the binary search runs the algorithm from scratch for `k` iterations.\n",
        "\n",
        "The iterations described below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqfY0p3PYIKr"
      },
      "source": [
        "\n",
        "#### First iteration\n",
        "\n",
        "1.  Iterate over every word and count pair in the input, denoted as `(w, c)`.\n",
        "2.  For each word `w`, generate every substring, denoted as `s`. E.g., for the\n",
        "    word `human`, we generate `{h, hu, hum, huma,\n",
        "    human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, ##n}`.\n",
        "3.  Maintain a substring-to-count hash map, and increment the count of each `s`\n",
        "    by `c`. E.g., if we have `(human, 113)` and `(humas, 3)` in our input, the\n",
        "    count of `s = huma` will be `113+3=116`.\n",
        "4.  Once we've collected the counts of every substring, iterate over the `(s,\n",
        "    c)` pairs *starting with the longest `s` first*.\n",
        "5.  Keep any `s` that has a `c \u003e T`. E.g., if `T = 100` and we have `(pers,\n",
        "    231); (dogs, 259); (##rint; 76)`, then we would keep `pers` and `dogs`.\n",
        "6.  When an `s` is kept, subtract off its count from all of its prefixes. This\n",
        "    is the reason for sorting all of the `s` by length in step 4. This is a\n",
        "    critical part of the algorithm, because otherwise words would be double\n",
        "    counted. For example, let's say that we've kept `human` and we get to\n",
        "    `(huma, 116)`. We know that `113` of those `116` came from `human`, and `3`\n",
        "    came from `humas`. However, now that `human` is in our vocabulary, we know\n",
        "    we will never segment `human` into `huma ##n`. So once `human` has been\n",
        "    kept, then `huma` only has an *effective* count of `3`.\n",
        "\n",
        "This algorithm will generate a set of word pieces `s` (many of which will be\n",
        "whole words `w`), which we *could* use as our WordPiece vocabulary.\n",
        "\n",
        "However, there is a problem: This algorithm will severely overgenerate word\n",
        "pieces. The reason is that we only subtract off counts of prefix tokens.\n",
        "Therefore, if we keep the word `human`, we will subtract off the count for `h,\n",
        "hu, hu, huma`, but not for `##u, ##um, ##uma, ##uman` and so on. So we might\n",
        "generate both `human` and `##uman` as word pieces, even though `##uman` will\n",
        "never be applied.\n",
        "\n",
        "So why not subtract off the counts for every *substring*, not just every\n",
        "*prefix*? Because then we could end up subtracting off the counts multiple\n",
        "times. Let's say that we're processing `s` of length 5 and we keep both\n",
        "`(##denia, 129)` and `(##eniab, 137)`, where `65` of those counts came from the\n",
        "word `undeniable`. If we subtract off from *every* substring, we would subtract\n",
        "`65` from the substring `##enia` twice, even though we should only subtract\n",
        "once. However, if we only subtract off from prefixes, it will correctly only be\n",
        "subtracted once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNCtKR8xT9wX"
      },
      "source": [
        "\n",
        "\n",
        "#### Second (and third ...) iteration\n",
        "\n",
        "To solve the overgeneration issue mentioned above, we perform multiple\n",
        "iterations of the algorithm.\n",
        "\n",
        "Subsequent iterations are identical to the first, with one important\n",
        "distinction: In step 2, instead of considering *every* substring, we apply the\n",
        "WordPiece tokenization algorithm using the vocabulary from the previous\n",
        "iteration, and only consider substrings which *start* on a split point.\n",
        "\n",
        "For example, let's say that we're performing step 2 of the algorithm and\n",
        "encounter the word `undeniable`. In the first iteration, we would consider every\n",
        "substring, e.g., `{u, un, und, ..., undeniable, ##n, ##nd, ..., ##ndeniable,\n",
        "...}`.\n",
        "\n",
        "Now, for the second iteration, we will only consider a subset of these. Let's\n",
        "say that after the first iteration, the relevant word pieces are:\n",
        "\n",
        "`un, ##deni, ##able, ##ndeni, ##iable`\n",
        "\n",
        "The WordPiece algorithm will segment this into `un ##deni ##able` (see the\n",
        "section [Applying WordPiece](#applying-wordpiece) for more information). In this\n",
        "case, we will only consider substrings that *start* at a segmentation point. We\n",
        "will still consider every possible *end* position. So during the second\n",
        "iteration, the set of `s` for `undeniable` is:\n",
        "\n",
        "`{u, un, und, unden, undeni, undenia, undeniab, undeniabl,\n",
        "undeniable, ##d, ##de, ##den, ##deni, ##denia, ##deniab, ##deniabl\n",
        ", ##deniable, ##a, ##ab, ##abl, ##able}`\n",
        "\n",
        "The algorithm is otherwise identical. In this example, in the first iteration,\n",
        "the algorithm produces the suprious tokens `##ndeni` and `##iable`. Now, these\n",
        "tokens are never considered, so they will not be generated by the second\n",
        "iteration. We perform several iterations just to make sure the results converge\n",
        "(although there is no literal convergence guarantee).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdUkqe84YQA5"
      },
      "source": [
        "### Applying WordPiece\n",
        "\n",
        "Once a WordPiece vocabulary has been generated, we need to be able to apply it\n",
        "to new data. The algorithm is a simple greedy longest-match-first application.\n",
        "\n",
        "For example, consider segmenting the word `undeniable`.\n",
        "\n",
        "We first lookup `undeniable` in our WordPiece dictionary, and if it's present,\n",
        "we're done. If not, we decrement the end point by one character, and repeat,\n",
        "e.g., `undeniabl`.\n",
        "\n",
        "Eventually, we will either find a subtoken in our vocabulary, or get down to a\n",
        "single character subtoken. (In general, we assume that every character is in our\n",
        "vocabulary, although this might not be the case for rare Unicode characters. If\n",
        "we encounter a rare Unicode character that's not in the vocabulary we simply map\n",
        "the entire word to `\u003cunk\u003e`).\n",
        "\n",
        "In this case, we find `un` in our vocabulary. So that's our first word piece.\n",
        "Then we jump to the end of `un` and repeat the processing, e.g., try to find\n",
        "`##deniable`, then `##deniabl`, etc. This is repeated until we've segmented the\n",
        "entire word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjRQKQzpYMl2"
      },
      "source": [
        "### Intuition\n",
        "\n",
        "Intuitively, WordPiece tokenization is trying to satisfy two different\n",
        "objectives:\n",
        "\n",
        "1.  Tokenize the data into the *least* number of pieces as possible. It is\n",
        "    important to keep in mind that the WordPiece algorithm does not \"want\" to\n",
        "    split words. Otherwise, it would just split every word into its characters,\n",
        "    e.g., `human -\u003e {h, ##u, ##m, ##a, #n}`. This is one critical thing that\n",
        "    makes WordPiece different from morphological splitters, which will split\n",
        "    linguistic morphemes even for common words (e.g., `unwanted -\u003e {un, want,\n",
        "    ed}`).\n",
        "\n",
        "2.  When a word does have to be split into pieces, split it into pieces that\n",
        "    have maximal counts in the training data. For example, the reason why the\n",
        "    word `undeniable` would be split into `{un, ##deni, ##able}` rather than\n",
        "    alternatives like `{unde, ##niab, ##le}` is that the counts for `un` and\n",
        "    `##able` in particular will be very high, since these are common prefixes\n",
        "    and suffixes. Even though the count for `##le` must be higher than `##able`,\n",
        "    the low counts of `unde` and `##niab` will make this a less \"desirable\"\n",
        "    tokenization to the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQZ38Uus-Xv1"
      },
      "source": [
        "## Optional: tf.lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NreDSRmJNG_h"
      },
      "source": [
        "If you need access to, or more control over the vocabulary it's worth noting that you can build the lookup table yourself and pass that to `BertTokenizer`.\n",
        "\n",
        "When you pass a string, `BertTokenizer` does the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thAF1DzQOQXl"
      },
      "outputs": [],
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.TextFileInitializer(\n",
        "        filename='pt_vocab.txt',\n",
        "        key_dtype=tf.string,\n",
        "        key_index = tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "        value_dtype = tf.int64,\n",
        "        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERY4FYN7O66R"
      },
      "source": [
        "Now you have direct access to the lookup table used in the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337_DcAMOs6N"
      },
      "outputs": [],
      "source": [
        "pt_lookup.lookup(tf.constant(['eÃÅ', 'um', 'uma', 'para', 'naÃÉo']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdZ82x5mPDE9"
      },
      "source": [
        "You don't need to use a vocabulary file, `tf.lookup` has other initializer options. If you have the vocabulary in memory you can use `lookup.KeyValueTensorInitializer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzkrmO9H-b9i"
      },
      "outputs": [],
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=pt_vocab,\n",
        "        values=tf.range(len(pt_vocab), dtype=tf.int64))) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrS1qVT5F3Hq"
      },
      "source": [
        "## Optional: Command line script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6KQg6F9oVYC"
      },
      "source": [
        "Note that there is a script that allow you to run the wordpiece_vocab learner from the command line if you have a counts file.\n",
        "\n",
        "The `text/tools/` directory needs to be in your python path for the script to work.\n",
        "\n",
        "Here's the builtin help:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHzL-ULlWEpd"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export PYTHONPATH=$PYTHONPATH:$PWD/text/tools\n",
        "\n",
        "python -m wordpiece_vocab.wordpiece_tokenizer_learner --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaTXLYn09DWU"
      },
      "source": [
        "\n",
        "There is a `wordpiece_tokenizer_learner.py` script that will take a counts file as input and produce a vocabulary file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKVMXNRJlZ_I"
      },
      "outputs": [],
      "source": [
        "pt_counts = count_words(train_examples.map(lambda pt, en: pt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EYSLDPmlJrD"
      },
      "outputs": [],
      "source": [
        "def write_counts(counts, filepath):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for word, count in counts.most_common():\n",
        "      print(word, count, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REJHv5RvlMiA"
      },
      "outputs": [],
      "source": [
        "write_counts(pt_counts, 'pt_counts.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-05P8-U-j_Z"
      },
      "source": [
        "It takes about 2 minutes to generate a vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAW8DFWv50SV"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "export PYTHONPATH=$PYTHONPATH:$PWD/text/tools\n",
        "\n",
        "time python -m wordpiece_vocab.wordpiece_tokenizer_learner \\\n",
        "  --vocab_size 8000 \\\n",
        "  --num_pad_tokens 1 \\\n",
        "  --reserved_tokens \"[START],[END]\" \\\n",
        "  --input_path pt_counts.txt \\\n",
        "  --output_path pt_vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFKtiPun9BH_"
      },
      "outputs": [],
      "source": [
        "!head pt_vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mk3yeGvYygT"
      },
      "outputs": [],
      "source": [
        "!wc -l pt_vocab.txt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "subwords_tokenizer.ipynb",
      "provenance": [
        {
          "file_id": "19NazJcl16cZ1xwhEh11C4x9NUwCCLHdH",
          "timestamp": 1603494039050
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_text/examples/subwords_tokenizer.ipynb?workspaceId=markdaoust:regression::citc",
          "timestamp": 1603479521744
        },
        {
          "file_id": "1XwKKU5xWmeTPjIsBiCfl0IY0_wyy5tBy",
          "timestamp": 1603406939256
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_text/examples/subwords_vocabulary.ipynb?workspaceId=markdaoust:regression::citc",
          "timestamp": 1603311119508
        },
        {
          "file_id": "1ztPVuzdLagq6gzaOBQyegejuOnvW4apD",
          "timestamp": 1603301876094
        },
        {
          "file_id": "/piper/depot/google3/third_party/tensorflow_text/examples/generate_vocabulary.ipynb?workspaceId=markdaoust:regression::citc",
          "timestamp": 1603297273233
        },
        {
          "file_id": "1oIG2IRO0IXzV6wbRaByVCSUfZem5T-T3",
          "timestamp": 1603233277012
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/tutorials/text/transformer.ipynb?workspaceId=markdaoust:regression::citc",
          "timestamp": 1603144867959
        },
        {
          "file_id": "15D1tXEy3cplTYsTIA7o-FVGBTPqfVLKD",
          "timestamp": 1603137485123
        },
        {
          "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/tutorials/text/transformer.ipynb?workspaceId=markdaoust:regression::citc",
          "timestamp": 1602635405008
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
